---
title: "A-Simplified-Explanation-of-AI-(and-How-It-Should-Be-Used)"
date: 2025-09-13
---

In May of 2017, the world’s best Go player Ke Jie was defeated in a 3-game match by AlphaGo, an AI model developed for the game by Google. The first game was close, but AlphaGo still pulled off a perfect 3-0 victory. This landmark match set the tone for AI in the Go world for several years. Match after match, AI reigned supreme, even as more models were released. Human players who had nearly spent all their lives learning and playing Go were soundly beat by AI.

And then in 2023, someone finally beat the AI, not just once, but 14 out of 15 times. The AI in question was a model called KataGo, capable of defeating top-level players. But the human player Kellin Pelrine, rather than a Go master, was an amateur player. What makes this story even stranger is that Pelrine employed a strategy that a Go master would easily counter.

How did this happen? As it happened, the AI had a blind spot to a particular and unusual strategy, and Pelrine was able to exploit it. KataGo got distracted by unimportant details of the game and started losing drastically when it thought it was winning.

So what caused this blind spot to pop up in KataGo’s program? To understand this, we need to have a basic understanding about how AI learns how to do anything at all. Whether it’s playing Go or predicting the next word in a sentence, AI is built upon one crucial element: data. The data given to an AI is the lifeblood needed for any valuable learning to take place.

If you were building an AI model designed to categorize road signs, you would need to be show it lots and lots of pictures of road signs (data), and then tell it to put these pictures in the corresponding categories. Eventually, the AI model would begin to record patterns in the pictures you give it. It would create rules based on these patterns like ‘triangle signs are more likely to be yield signs’ or ‘pictures with a big zero at the bottom right are likely speed limits’. Once this happens enough times, the AI (hopefully) can be given a picture of a stop sign, and it will categorize the picture as ‘stop sign’ without needing to be told what category it belongs to. Good job! Your model can categorize road signs, not an easy feat!

That was a pretty big simplification of how AI learns things, but it’s enough to talk about what happened with KataGo. In order for KataGo to win at a game, the programmers at Google exposed it to thousands of games of Go and told it what the best moves were in each situation. (Again, this explanation is simplified.) You might already be able to see why KataGo lost. Pelrine used a strategy that doesn’t come up in very many games between human players, so that strategy wasn’t prevalent in the data used to train KataGo. And so it got confused, and focused on insignificant parts of the game while Pelrine won.

AI models are only as good as the data used to train them. When there’s little to no data about a particular situation, an AI model simply isn’t reliable to make the correct decision when that situation comes up. Because of this, rare and novel situations are a real thorn in the side of many AI models. They cause problems with all sorts of models, whether they are used for geopolitics, cybersecurity, science, culture, or any other topic where new and novel situations appear. Even new slang creates temporary blind spots in models that deal with language and conversations, like ChatGPT.

There are two big points I want you to take away from this post: one cautionary, and one optimistic.

First, as the development and evolution of AI continues, we should keep in mind what AI is and is not good at doing. It’s not a silver bullet we can apply for every situation, even though we are still only scratching the surface of what it can do. Even a model made by Google to play Go on a superhuman level can flounder when faced with the unexpected. There’s a lot of talk about ‘replacing human workers with AI’. What might happen if we get carried away with that idea? Instead of losing Go games because of unusual strategies, corporations using AI to drive business decisions could lose millions due to an irregularity in the economy. Hospitals using AI with little human oversight could be paralyzed during a worldwide pandemic because that just isn’t what the models were trained to handle. Let us be optimistic about the potential good AI can do, but let us be slow to apply it in fields that require flexible, adaptive, original thinking. For now at least, humans are the superior species in that regard.

Second, AI is going to impact our future even more than it has impacted our present. There is cause to be wary of what that future may be, but I also see a great potential benefit in the way AI could shift the way we focus our labor. After all, if originality is one of AI’s weaknesses, then the demand for workers who can think originally will naturally go up as AI becomes more ubiquitous. I for one am hopeful that this will result in a future where less manpower is spent doing the monotonous and tedious grindwork present in almost every workplace, and more time is spent growing, learning, and finding new solutions to new problems. More time and resources could be devoted to human enrichment and improvement as a matter of economic practicality. That is a future I can get behind!
